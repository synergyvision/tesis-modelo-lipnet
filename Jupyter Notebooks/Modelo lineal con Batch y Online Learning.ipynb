{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajuste de un modelo lineal\n",
    "\n",
    "Este ejemplo muestra el proceso de ajuste de un modelo lineal con un perceptrón simple y como función de activación la función lineal. Se utilizarán los métodos de aprendizaje automático `Batch Learning` y `Online Learning`. Los cuales se detallarán más adelante.\n",
    "\n",
    "El perceptrón recibe $x$ y el sesgo $b$ como entrada y se utiliza una tasa de aprendizaje de $0,05$. Los datos corresponden a la función lineal $f(x)=2x-10$. Se quiere conseguir mediante el método del descenso del gradiente los dos parámetros $w1$ y $b1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la realización del modelo solo se utilizarán las librerías `statics` y `numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics as st\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos de entrada son de la función lineal descrita y se utilizan 13 datos para entrenar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos de entrada\n",
    "xList = [-5,  -4,  -3,  -2,  -1,   0,  1,  2,  3,  4, 5, 6, 7]\n",
    "\n",
    "# Datos de salida (la función es 2*x - 10)\n",
    "yList = [-20, -18, -16, -14, -12, -10, -8, -6, -4, -2, 0, 2, 4]\n",
    "\n",
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se definen las funciones que utilizarán los métodos de aprendizaje:\n",
    "\n",
    "- `linear` es la función de activación.\n",
    "- `forward` es la fase de estimación que le aplica la función de activación a la entrada multiplicada por el parámetro y suma el sesgo.\n",
    "- `backward` es la fase de ajuste del parámetro mediante el descenso del gradiente, que utiliza la tasa de aprendizaje `alpha` y el gradiente. El gradiente es la derivada de la función de error con respecto al parámetro, que en nuestro caso es la función de error cuadrático.\n",
    "- `error` es la función que calcula el error cuadrático entre el valor estimado y el valor observado en los datos. Para este caso la función de error es: $\\sum_{i=1}^{n}(\\hat{y_{i}}-y_{i})^{^{2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, w, b):\n",
    "    yEst = linear(w*x + b)\n",
    "    return yEst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(w, dE_w):\n",
    "    return w - (alpha*dE_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(yEst, y):\n",
    "    return np.power(yEst-y, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Más adelante se definen las funciones `batch_learning` y `online_learning`, estos dos métodos presentan variables en común. En particular la condición de parada para el proceso de ajuste iterativo en ambos es $10^{-20}$. Los valores iniciales de los parámetros $w1$ y $b1$ son arbitrarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Condición de parada\n",
    "stopAt = 1e-20\n",
    "\n",
    "#Peso y sesgo incial arbitrarios\n",
    "w1 = 0.46\n",
    "b1 = 0.27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Learning\n",
    "Este método de aprendizaje automático consiste en procesar los datos de entrada por lotes, es decir, se utilizan los mismos parámetros $w1$ y $b1$ para un lote de los datos de entrada y solo después de calcular cada uno de las valores estimados y sus errores, se ajustan los parámetros y se sigue con el siguiente lote, así sucesivamente, hasta cumplir con la condición de parada o superar el número de iteraciones.\n",
    "\n",
    "La funcion que se definió para este método fue `batch_learning(x, y, n_batch, numIter)`, donde `x` representa los datos de entrada, `y` los datos de salida, `n_batch` el número de datos por lote y `numIter` el número de iteraciones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es el código principal de la función, aquí se dividen los datos de entrada en lotes de acuerdo a la variable `n_batch`, luego está  el ciclo principal que se ejecuta dentro de la función a lo sumo `numIter` veces. Se define una variable `i` para saber cual lote de los datos tomar y cual lote de los valores observados tomar para calcular los diferentes errores y luego ajustar los parámetros.\n",
    "\n",
    "El proceso consiste en el siguiente ciclo:\n",
    "\n",
    "- Realizar el paso de estimación denominado `forward`.\n",
    "- Calcular el error cuadrático entre el valor estimado en la fase previa y el valor observado mediante la función `error`.\n",
    "- Calcular los gradientes que consiste en calcular la derivada de la función de error con respecto a `w1` y con respecto a `b1`.\n",
    "- Guardar los gradientes. \n",
    "- Aplicar el paso de ajuste `backward`, con el promedio de los gradientes almacenados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_learning(x, y, n_batch, numIter):\n",
    "    echo = False\n",
    "    #Condición de parada\n",
    "    stopAt = 1e-20\n",
    "    \n",
    "    #Peso y sesgo incial arbitrarios\n",
    "    w1 = 0.46\n",
    "    b1 = 0.27\n",
    "    \n",
    "    #Separando los datos en lotes\n",
    "    xBatch = [x[i:i + n_batch] for i in range(0, len(x), n_batch)]\n",
    "    yBatch = [y[i:i + n_batch] for i in range(0, len(y), n_batch)]\n",
    "    \n",
    "    for i in range(numIter):\n",
    "        j = i%len(xBatch)\n",
    "        \n",
    "        w1_Derivatives = []\n",
    "        b1_Derivatives = []\n",
    "        errors = []\n",
    "        for index in range(len(xBatch[j])):\n",
    "            x = xBatch[j][index]\n",
    "            y = yBatch[j][index]\n",
    "            if echo:\n",
    "                # Imprimir valores\n",
    "                print('Iteración '+str(i+1)+str(' de ')+str(numIter))\n",
    "                print('w1:'+str(w1)+', b1:'+str(b1))\n",
    "            \n",
    "            # Cálculo de y Estimado \n",
    "            yEst = forward(x, w1, b1)\n",
    "            if echo:\n",
    "                print('y:'+str(y)+', yEst:'+str(yEst))\n",
    "    \n",
    "            # Cálculo del error\n",
    "            sse = error(yEst, y)\n",
    "            if echo:\n",
    "                print('error '+str(sse))\n",
    "                \n",
    "            # Cálculo de las derivadas parciales del Error con \n",
    "            # respecto a los parámetros\n",
    "            dE_w1 = 2*(yEst-y)*x\n",
    "            dE_b1 = 2*(yEst-y)\n",
    "            \n",
    "            if echo:\n",
    "                print('d3_w1:'+str(dE_w1)+', dE_b1:'+str(dE_b1))\n",
    "            \n",
    "            #Guardar los gradientes\n",
    "            w1_Derivatives.append(dE_w1)\n",
    "            b1_Derivatives.append(dE_b1)\n",
    "            errors.append(sse)\n",
    "        \n",
    "        #Promedio de los gradientes\n",
    "        dE_w1Batch = st.mean(w1_Derivatives)\n",
    "        dE_b1Batch = st.mean(b1_Derivatives)\n",
    "        \n",
    "        # Ajustando los parámetros w1 y b1\n",
    "        w1 = backward(w1, dE_w1Batch)\n",
    "        b1 = backward(b1, dE_b1Batch)\n",
    "        if (st.mean(errors)<stopAt):\n",
    "            return b1,w1\n",
    "    return b1,w1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprobar el funcionamiento de la función la invocamos con los datos de entrada, los datos de salida, el \n",
    "número de lotes igual a `2` y un número máximo de iteraciones igual a `500`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.999999999704968 1.9999999999142035\n"
     ]
    }
   ],
   "source": [
    "b1, w1 = batch_learning(xList, yList, 2,500)\n",
    "print(b1,w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Learning\n",
    "Esta forma de aprendizaje automático a diferencia de `Batch Learning` no utiliza lotes de datos para ajustar los parámetros, simplemente procesa paso por paso cada dato de entrada, calcula el valor estimado y su error, y justo después ajusta los parámetros.\n",
    "\n",
    "En en la función básicamente se sigue el mismo proceso que `batch_learning` solo que no se guardan los gradientes y no se utiliza el promedio en ningún momento.\n",
    "\n",
    "El proceso consiste en el siguiente ciclo:\n",
    "\n",
    "- Realizar el paso de estimación denominado `forward`.\n",
    "- Calcular el error cuadrático entre el valor estimado en la fase previa y el valor observado mediante la función `error`.\n",
    "- Calcular los gradientes que consiste en calcular la derivada de la función de error con respecto a `w1` y con respecto a `b1`.\n",
    "- Aplicar el paso de ajuste `backward`, directamente con el gradiene previamente calculado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_learning():\n",
    "    echo = False\n",
    "    #Condición de parada\n",
    "    stopAt = 1e-20\n",
    "    \n",
    "    #Peso y sesgo incial arbitrarios\n",
    "    w1 = 0.46\n",
    "    b1 = 0.27\n",
    "    \n",
    "    numIter = 500\n",
    "    sizeOfData = np.size(xList)\n",
    "    \n",
    "    for i in range(numIter):\n",
    "        \n",
    "        index = i%sizeOfData\n",
    "        x = xList[index]\n",
    "        y = yList[index]\n",
    "        \n",
    "        if echo:\n",
    "            # Imprimir valores\n",
    "            print('Iteración '+str(i+1)+str(' de ')+str(numIter))\n",
    "            print('Datos: x'+str(index)+':'+str(x)+', w1:'+str(w1)+', b1:'+str(b1))\n",
    "        \n",
    "        # Cálculo de y Estimado\n",
    "        yEst = forward(x, w1, b1)\n",
    "        if echo:\n",
    "            print('y:'+str(y)+', yEst:'+str(yEst))\n",
    "        \n",
    "        # Cálculo del Error\n",
    "        sse = error(yEst, y)\n",
    "        if echo:\n",
    "            print('error '+str(sse))\n",
    "        \n",
    "        # Cálculo de las derivadas parciales\n",
    "        dE_w1 = 2*(yEst-y)*x\n",
    "        dE_b1 = 2*(yEst-y)\n",
    "        if echo:\n",
    "            print('d3_w1:'+str(dE_w1)+', dE_b1:'+str(dE_b1))\n",
    "        \n",
    "        # Ajustando los parámetros w1 y b1\n",
    "        w1 = backward(w1, dE_w1)\n",
    "        b1 = backward(b1, dE_b1)\n",
    "        if echo:\n",
    "            print('After adjusting')\n",
    "            print('w1:'+str(w1)+', b1:'+str(b1))\n",
    "            print('')\n",
    "        \n",
    "        # Chequear condición de parada\n",
    "        if (sse<stopAt):\n",
    "            return b1, w1\n",
    "    return b1, w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.999999997782723 1.9999999995335829\n"
     ]
    }
   ],
   "source": [
    "b1, w1 = online_learning()\n",
    "print(b1,w1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
